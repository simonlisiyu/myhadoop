//package com.lsy.myhadoop.spark2.test1.reco.realtime
//
//import java.util.Properties
//import kafka.javaapi.producer.Producer
//import kafka.producer.KeyedMessage
//import kafka.producer.ProducerConfig
//
//object BehaviorsKafkaProducer {
//
//  val newClickEvents= Seq(
//    (1000000L, 123L),
//    (1000001L, 400L),
//    (1000002L, 500L),
//    (1000003L, 278L),
//    (1000004L, 681L)
//  )
//
//  def run(topic: String) {
//    val props: Properties = new Properties
//    props.put("metadata.broker.list", "ip123.com")
//    props.put("serializer.class", "kafka.serializer.DefaultEncoder")
//    val conf: ProducerConfig = new ProducerConfig(props)
//    var producer: Producer[String, Array[Byte]] = null
//    try {
//      System.out.println("Producing messages")
//      producer = new Producer[String, Array[Byte]](conf)
//      for (event <- newClickEvents) {
//        val eventProto = NewClickEvent.newBuilder().setUserId(event._1).setItemId(event._2).build()
//        producer.send(new KeyedMessage[String, Array[Byte]](topic, eventProto.toByteArray))
//        print("Sending messages:" + eventProto.toString)
//      }
//      println("Done sending messages")
//    } catch {
//      case ex: Exception => {
//        println("Error while producing messagesï¼š" + ex)
//      }
//    } finally {
//      if (producer != null) producer.close
//    }
//  }
//
//  @throws(classOf[Exception])
//  def main(args: Array[String]) {
//    BehaviorsKafkaProducer.run(KafkaRedisProperties.KAFKA_RECO_TOPIC)
//  }
//}
